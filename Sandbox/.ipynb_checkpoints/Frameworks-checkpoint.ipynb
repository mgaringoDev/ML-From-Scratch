{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a quick overview of what I will be talking about in this section.  Which is basically comparing Numpy vs TensorFlow vs. PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overviewIMG](docs/comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a two-layer ReLu Netowrk on random data with L2 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inefficient Method\n",
    "\n",
    "CPU GPU bottle-neck because we are copying CPU and GPU everytime step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Computational Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,D,H = 64, 1000, 1000\n",
    "# These are the input nodes of the graph and are fed into the graph\n",
    "# These are not allocating to memory\n",
    "x = tf.placeholder(tf.float32,shape=(N,D))\n",
    "y = tf.placeholder(tf.float32,shape=(N,D))\n",
    "w1 = tf.placeholder(tf.float32,shape=(D,H))\n",
    "w2 = tf.placeholder(tf.float32,shape=(H,D))\n",
    "\n",
    "# Using these symbolic values we will perform tensor operations on the.  \n",
    "# In this case forward propagation.\n",
    "h = tf.maximum(tf.matmul(x,w1),0) # matrix multiplacation\n",
    "y_pred = tf.matmul(h,w2) # ReLU\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2,axis=1)) # L2 loss\n",
    "\n",
    "# This is as tensor flow to calculate the gradients of the loss wrt to w1 and w2\n",
    "# This avoids creating back-prop code\n",
    "grad_w1, grad_w2 = tf.gradients(loss,[w1,w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xe2d8f60>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entering a tensor flow session\n",
    "lossRunning = []\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # These are finally concrete values into the graph.  \n",
    "    # These are fed in using numpy\n",
    "    values = {x:np.random.randn(N,D),\n",
    "              w1:np.random.randn(D,H),\n",
    "              w2:np.random.randn(H,D),\n",
    "              y:np.random.randn(N,D)\n",
    "             }\n",
    "    \n",
    "    #Hyper-Parameters\n",
    "    learning_rate = 1e-5\n",
    "    numItr = 50\n",
    "    \n",
    "    # Training the model\n",
    "    for itr in range(numItr):\n",
    "        # In this section we are basically saying we want to calculate loss,dw1,dw2\n",
    "        # and feeding in a dictionary of values which were specified by the \n",
    "        # palceholders\n",
    "        out = sess.run([loss,grad_w1,grad_w2],feed_dict = values)\n",
    "\n",
    "        # inpack all the output and you get the loss and the gradients\n",
    "        loss_val,grad_w1_val,grad_w2_val = out\n",
    "        lossRunning.append(loss_val)\n",
    "        \n",
    "        # Update the weights\n",
    "        values[w1] -= learning_rate *grad_w1_val\n",
    "        values[w2] -= learning_rate *grad_w2_val\n",
    "\n",
    "plt.plot(lossRunning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,D,H = 64, 1000, 1000\n",
    "# These are the input nodes of the graph and are fed into the graph\n",
    "# These are not allocating to memory\n",
    "x = tf.placeholder(tf.float32,shape=(N,D))\n",
    "y = tf.placeholder(tf.float32,shape=(N,D))\n",
    "w1 = tf.Variable(tf.random_normal((D,H)))\n",
    "w2 = tf.Variable(tf.random_normal((H,D)))\n",
    "\n",
    "# Using these symbolic values we will perform tensor operations on the.  \n",
    "# In this case forward propagation.\n",
    "h = tf.maximum(tf.matmul(x,w1),0) # matrix multiplacation\n",
    "y_pred = tf.matmul(h,w2) # ReLU\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2,axis=1)) # L2 loss\n",
    "\n",
    "# This is as tensor flow to calculate the gradients of the loss wrt to w1 and w2\n",
    "# This avoids creating back-prop code\n",
    "grad_w1, grad_w2 = tf.gradients(loss,[w1,w2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
