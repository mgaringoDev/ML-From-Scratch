{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The following was inspired by the following articles\n",
    "\n",
    "- Medium\n",
    "    - How to build a Recurrent Neural Network in TensorFlow [[1]](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767)[[2]](https://medium.com/@erikhallstrm/tensorflow-rnn-api-2bb31821b185)[[3]](https://medium.com/@erikhallstrm/using-the-tensorflow-lstm-api-3-7-5f2b97ca6b73)[[4]](https://medium.com/@erikhallstrm/using-the-tensorflow-multilayered-lstm-api-f6e7da7bbe40)[[5]](https://medium.com/@erikhallstrm/using-the-dynamicrnn-api-in-tensorflow-7237aba7f7ea)[[6]](https://medium.com/@erikhallstrm/using-the-dropout-api-in-tensorflow-2b2e6561dfeb)  \n",
    "    - [RNN example by Python](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470)       \n",
    "- GitRepos\n",
    "    - [char-rnn-tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow)\n",
    "    - [RNN](https://github.com/WillKoehrsen/recurrent-neural-networks/tree/master/notebooks)\n",
    "- Kaggle Repos\n",
    "    - [Learn by example RNN/LSTM/GRU time series](https://www.kaggle.com/charel/learn-by-example-rnn-lstm-gru-time-series)\n",
    "- machinelearningmaster\n",
    "    - [How to Develop a Character-Based Neural Language Model in Keras](https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/)\n",
    "    - [Adventures of machine learning](http://adventuresinmachinelearning.com/keras-lstm-tutorial/)\n",
    "-  Troubleshooting\n",
    "    - [Input size of the LSTM layer](https://github.com/keras-team/keras/issues/2045)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# A regular expression is a special sequence of characters that helps you match or find other strings or sets of strings, \n",
    "# using a specialized syntax held in a pattern.\n",
    "# https://www.tutorialspoint.com/python/python_reg_expressions.htm\n",
    "import re\n",
    "\n",
    "# KERAS\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# SKlearn\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses the re package\n",
    "def format_patent(patent):\n",
    "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
    "\n",
    "    # Add spaces around punctuation\n",
    "    # this says take any of the characters and substitute it with a space\n",
    "    # basically gets rid of the special characters\n",
    "    patent = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', patent)\n",
    "\n",
    "    # Remove references to figures\n",
    "    # removes any characters with the sequence (#)\n",
    "    patent = re.sub(r'\\((\\d+)\\)', r'', patent)\n",
    "\n",
    "    # Remove double spaces    \n",
    "    patent = re.sub(r'\\s\\s', ' ', patent)\n",
    "    return patent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(texts,\n",
    "                   training_length=50,\n",
    "                   lower=True,\n",
    "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "\n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Create look-up dictionaries and reverse look-ups    \n",
    "    word_idx = tokenizer.word_index # for each word there is an associated index\n",
    "    idx_word = tokenizer.index_word # for each index there is an associated word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "\n",
    "    print(f'There are {num_words} unique words.')\n",
    "\n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [\n",
    "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
    "    ]\n",
    "\n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "\n",
    "    # Only keep sequences with more than training length tokens\n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])\n",
    "\n",
    "    training_seq = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through the sequences of tokens\n",
    "    for seq in new_sequences:\n",
    "\n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # Extract the features and label\n",
    "            extract = seq[i - training_length:i + 1]\n",
    "\n",
    "            # Set the features and label\n",
    "            training_seq.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "\n",
    "    print(f'There are {len(training_seq)} training sequences.')\n",
    "\n",
    "    # Return everything needed for setting up the model\n",
    "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, training_seq, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding and Train/Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_valid(features,\n",
    "                       labels,\n",
    "                       num_words,\n",
    "                       train_fraction=TRAIN_FRACTION):\n",
    "    \"\"\"Create training and validation features and labels.\"\"\"\n",
    "\n",
    "    # Randomly shuffle features and labels\n",
    "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Decide on number of samples for training\n",
    "    train_end = int(train_fraction * len(labels))\n",
    "\n",
    "    train_features = np.array(features[:train_end])\n",
    "    valid_features = np.array(features[train_end:])\n",
    "\n",
    "    train_labels = labels[:train_end]\n",
    "    valid_labels = labels[train_end:]\n",
    "\n",
    "    # Convert to arrays\n",
    "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
    "\n",
    "    # Using int8 for memory savings\n",
    "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
    "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
    "\n",
    "    # One hot encoding of labels\n",
    "    for example_index, word_index in enumerate(train_labels):\n",
    "        y_train[example_index, word_index] = 1\n",
    "\n",
    "    for example_index, word_index in enumerate(valid_labels):\n",
    "        y_valid[example_index, word_index] = 1\n",
    "\n",
    "    # Memory management\n",
    "    import gc\n",
    "    gc.enable()\n",
    "    del features, labels, train_features, valid_features, train_labels, valid_labels\n",
    "    gc.collect()\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 50\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 2048\n",
    "TRAINING_LENGTH = 50\n",
    "TRAIN_FRACTION = 0.7\n",
    "LSTM_CELLS = 64\n",
    "VERBOSE = 0\n",
    "SAVE_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location\n",
    "fileDir = 'data/rnn/neural_network_patent_query.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper- parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data = pd.read_csv(\n",
    "    fileDir, parse_dates=['patent_date'])\n",
    "\n",
    "# Extract abstracts\n",
    "original_abstracts = list(data['patent_abstract'])\n",
    "len(original_abstracts)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} patents in total with {} coloumns with the following information \\n {}'.format(data.shape[0],data.shape[1],data.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted = []\n",
    "\n",
    "# Iterate through all the original abstracts\n",
    "for a in original_abstracts:\n",
    "    formatted.append(format_patent(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Text to Sequence\n",
    "\n",
    "The main purpose is to divide the entire abstract information into sentences with **n** number of words.  Where each word is a number which corresponds to the vocabulary.\n",
    "\n",
    "Our preprocessing is going to involve using a Tokenizer to convert the patents from sequences of words (strings) into sequences of integers.\n",
    "\n",
    "We will use a sequencing function to perform this task.  Where this function takes various parameters including a training length which is the number of words we will feed into the network as features with the next word the label. For example, if we set training_length = 50, then the model will take in 50 words as features and the 51st word as the label.\n",
    "\n",
    "For each abstract, we can make multiple training examples by slicing at different points. We can use the first 50 words as features with the 51st as a label, then the 2nd through 51st word as features and the 52nd as the label, then 3rd - 52nd with 53rd as label and so on. This gives us much more data to train on and the performance of the model is proportional to the amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_LENGTH = 50\n",
    "filters = '!\"#$%&()*+/:<=>@[\\\\]^_`{|}~\\t\\n'\n",
    "word_idx, idx_word, num_words, word_counts, abstracts, sequences, features, labels = make_sequences(\n",
    "    formatted, TRAINING_LENGTH, lower=True, filters=filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the above step as creating a feature matrix wit the following dimensions **[sentence][word]**.\n",
    "\n",
    "Where each sentence contains 50 words in which each word corresponds to a value in the vocabulary.\n",
    "\n",
    "The vocabulary is index by two variables\n",
    "- word_idx\n",
    "- idx_word\n",
    "\n",
    "There are two varibles above which we can use as statistics and they are:\n",
    "- num_words\n",
    "- word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the RNN - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split a the data into training and validation set and use one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = create_train_valid(\n",
    "    features, labels, num_words)\n",
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
