{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is my vanilla RNN implementation.  I got helped from Dr.Karpathy of the Stanford 231n profs.  The github implementation can be found [here](https://gist.github.com/karpathy/d4dee566867f8291f086).  And the original blog post based on this can be found [here](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "\n",
    "This is an extension of RNN but with better gradient management.\n",
    "\n",
    "I used the following resources:\n",
    "- [Deriving LSTM Gradient for Backpropagation](https://wiseodd.github.io/techblog/2016/08/12/lstm-backprop/)\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Karpathy Git Repo LSTM](https://gist.github.com/karpathy/587454dc0146a6ae21fc)\n",
    "- [Vector,Matrix and Tensor Derivatives Cheatsheet](http://cs231n.stanford.edu/vecDerivs.pdf)\n",
    "- [hackernoon: Understanding architecture of LSTM cell from scratch with code.](https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4)\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Simple LSTM](http://nicodjimenez.github.io/2014/08/08/lstm.html)\n",
    "- [Simple LSTM Code](https://github.com/nicodjimenez/lstm/blob/master/lstm.py)\n",
    "- [Only Numpy](http://blog.varunajayasiri.com/numpy_lstm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get the data from a text file\n",
    "- get the list of characters\n",
    "- get the length of the db and the vocabulary\n",
    "- associate each character with an index and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the database from a list of characters that is found in the input.txt\n",
    "# should be simple plain text file\n",
    "data = open('input.txt', 'r').read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of characters by first identifying the set() of characters and place them in a list()\n",
    "# NOTE: set() function here creates an unorders collection with no duplicate elements\n",
    "chars = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The characters that were detected were:['\\n', ' ', \"'\", '-', 'A', 'F', 'I', 'H', 'N', 'P', 'S', 'T', 'W', 'Y', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y']\n"
     ]
    }
   ],
   "source": [
    "print('The characters that were detected were:{}'.format(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the size of the data and the size of the vocabulary we have\n",
    "data_size, vocab_size = len(data), len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1681 characters, 37 unique.\n"
     ]
    }
   ],
   "source": [
    "print 'data has {} characters, {} unique.'.format(data_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a text file so we know that there maybe 26 characters from the alphabet * 2 because of the capitals plus punctuations.  This seems reasonable so let us continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate the characters and give indices to them\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " \"'\": 2,\n",
       " '-': 3,\n",
       " 'A': 4,\n",
       " 'F': 5,\n",
       " 'H': 7,\n",
       " 'I': 6,\n",
       " 'N': 8,\n",
       " 'P': 9,\n",
       " 'S': 10,\n",
       " 'T': 11,\n",
       " 'W': 12,\n",
       " 'Y': 13,\n",
       " 'a': 14,\n",
       " 'b': 16,\n",
       " 'c': 15,\n",
       " 'd': 18,\n",
       " 'e': 17,\n",
       " 'f': 20,\n",
       " 'g': 19,\n",
       " 'h': 22,\n",
       " 'i': 21,\n",
       " 'j': 24,\n",
       " 'k': 23,\n",
       " 'l': 26,\n",
       " 'm': 25,\n",
       " 'n': 28,\n",
       " 'o': 27,\n",
       " 'p': 29,\n",
       " 'r': 31,\n",
       " 's': 30,\n",
       " 't': 33,\n",
       " 'u': 32,\n",
       " 'v': 35,\n",
       " 'w': 34,\n",
       " 'y': 36}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: \"'\",\n",
       " 3: '-',\n",
       " 4: 'A',\n",
       " 5: 'F',\n",
       " 6: 'I',\n",
       " 7: 'H',\n",
       " 8: 'N',\n",
       " 9: 'P',\n",
       " 10: 'S',\n",
       " 11: 'T',\n",
       " 12: 'W',\n",
       " 13: 'Y',\n",
       " 14: 'a',\n",
       " 15: 'c',\n",
       " 16: 'b',\n",
       " 17: 'e',\n",
       " 18: 'd',\n",
       " 19: 'g',\n",
       " 20: 'f',\n",
       " 21: 'i',\n",
       " 22: 'h',\n",
       " 23: 'k',\n",
       " 24: 'j',\n",
       " 25: 'm',\n",
       " 26: 'l',\n",
       " 27: 'o',\n",
       " 28: 'n',\n",
       " 29: 'p',\n",
       " 30: 's',\n",
       " 31: 'r',\n",
       " 32: 'u',\n",
       " 33: 't',\n",
       " 34: 'w',\n",
       " 35: 'v',\n",
       " 36: 'y'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "concatHiddenLayerInputLayerSize = vocab_size + hidden_size\n",
    "\n",
    "Wih=np.random.randn(concatHiddenLayerInputLayerSize, hidden_size) \n",
    "Wfh=np.random.randn(concatHiddenLayerInputLayerSize, hidden_size) \n",
    "Woh=np.random.randn(concatHiddenLayerInputLayerSize, hidden_size) \n",
    "Wgh=np.random.randn(concatHiddenLayerInputLayerSize, hidden_size) \n",
    "Wch=np.random.randn(concatHiddenLayerInputLayerSize, hidden_size) \n",
    "Wyh=np.random.randn(hidden_size, vocab_size) \n",
    "\n",
    "bi=np.zeros((1, hidden_size))\n",
    "bf=np.zeros((1, hidden_size))\n",
    "bo=np.zeros((1, hidden_size))\n",
    "bg=np.zeros((1, hidden_size))\n",
    "bc=np.zeros((1, hidden_size))\n",
    "by=np.zeros((1, hidden_size))\n",
    "\n",
    "\n",
    "# Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "# Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "# Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "# bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "# by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    sigmoidOut = 1 / (1 + np.exp(-x))    \n",
    "    return sigmoidOut  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsigmoid(x):\n",
    "    dsigmoidOut = sigmoid(x)*(1 - sigmoid(x))\n",
    "    return dsigmoidOut  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    tanhOut = np.tanh(x)\n",
    "    return tanhOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtanh(x):\n",
    "    dtanh = 1 - tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pass**\n",
    "![](https://i.imgur.com/qaJLCZK.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers of size seq_length \n",
    "    hprev is [Hx1] array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    # inits for forward pass\n",
    "    # xs: input state\n",
    "    # hs: hidden state\n",
    "    # ys: output state\n",
    "    # ps: propbability state\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "\n",
    "    # forward pass\n",
    "    # t is the tick for all inputs in the sequence\n",
    "    for t in xrange(len(inputs)):\n",
    "        # one-hot-encoder\n",
    "        # get zeros\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        # place a one where the input index is\n",
    "        xs[t][inputs[t]] = 1\n",
    "\n",
    "        # obtain the current hidden state by using the regular equation for RNN\n",
    "        # ht = tanh(whh*ht-1 + wxh*xt) where * is a dot operation\n",
    "        # in other words \n",
    "        # multiply the weigths with the previous hidden state, \n",
    "        # add the weigths times the input of the current tick, \n",
    "        # add some bias\n",
    "        # apply a non-linarity in this case tanh\n",
    "        # this produces the current hidden state\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "\n",
    "        # obtain the output stae by applying the dot product of the input weigths with the hidden state and adding a bias\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "\n",
    "        # loss function which is the negative log cross-entropy\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "    # inits for backward pass\n",
    "    # backprop: compute gradients going backwards\n",
    "    # dWxh: the gradient of the weights between the input and hidden layer\n",
    "    # dWhh: the gradient of the weights in the hidden layer\n",
    "    # dWhy: the gradient of the weights between the hidden and output layer\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    # dbh: the bias gradient of the hidden layer\n",
    "    # dby: the bias gradient of the output layer\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    # dhnext: the running gradient while propagarting through time.  If you don't see it refer to the computational graph.\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "    # going backwards for each t is the tick for all inputs in the sequence. Again look at the computational graph.\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        # upstream through the loss function\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "        # upstream through the weigths    \n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy    \n",
    "\n",
    "        # upstream through tanh (derivative of tanh)\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "\n",
    "        # upstream through stack (see computational graph)\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "\n",
    "        # upstream thought the next input\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    # Mitigate exploding and vanishing gradients.\n",
    "    # if the largest singular value > 1 in any of the gradients then you would slowly increase to infinity and explode\n",
    "    # if the largest singular value < 1 in any of the gradients then you would slowly decrease to 0 and disappear\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "\n",
    "    # return the following\n",
    "    # [loss]: for logging\n",
    "    # [dWxh, dWhh, dWhy, dbh, dby]: grardients for input,hidden,output weigts and biases\n",
    "    # [hs]: the current hidden state which will be the previous state for the next set of sequence in the backprop thourgh time\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample\n",
    "Get sample from the output distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is the current memory state, \n",
    "  seed_ix is seed letter for first time step ... basically the starting seed to generate the text\n",
    "  n is the number of characters to generate\n",
    "  \"\"\"\n",
    "  \n",
    "  # intialization  \n",
    "  # one-hot encode the seed\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "\n",
    "  # running array list for the outputs\n",
    "  ixes = []\n",
    "\n",
    "  # go through each tick to generate the next character in all of the n generated characters  \n",
    "  for t in xrange(n):\n",
    "    # forward pass\n",
    "    \n",
    "    # calculate hidden state\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    \n",
    "    # calculate output\n",
    "    y = np.dot(Why, h) + by\n",
    "    \n",
    "    # calculate probabilities\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    \n",
    "    # select a random with some probability associated with values\n",
    "    # np.random.choice(allPossibileOutcomes, probabilityOfTheOutcomes)\n",
    "    # choose a number in the vocab based on the probabilty distribution found in previous step\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    \n",
    "    # one-hot encode this in a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    \n",
    "    # append running vector outputs\n",
    "    ixes.append(ix)\n",
    "  return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample(h, seed_ix, n):\n",
    "#   \"\"\" \n",
    "#   sample a sequence of integers from the model \n",
    "#   h is memory state, seed_ix is seed letter for first time step\n",
    "#   \"\"\"\n",
    "#   x = np.zeros((vocab_size, 1))\n",
    "#   x[seed_ix] = 1\n",
    "#   ixes = []\n",
    "#   for t in xrange(n):\n",
    "#     h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "#     y = np.dot(Why, h) + by\n",
    "#     p = np.exp(y) / np.sum(np.exp(y))\n",
    "#     ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "#     x = np.zeros((vocab_size, 1))\n",
    "#     x[ix] = 1\n",
    "#     ixes.append(ix)\n",
    "#   return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of the data index as well as the iteration \n",
    "# n = interation counter\n",
    "# p = data pointer\n",
    "n, p = 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the weights all to zero\n",
    "Wih=np.random.randn(concatHiddenLayerInputLayerSize, hidden_size) \n",
    "Wfh=np.random.randn(concatHiddenLayerInputLayerSize, hidden_size) \n",
    "Woh=np.random.randn(concatHiddenLayerInputLayerSize, hidden_size) \n",
    "Wgh=np.random.randn(concatHiddenLayerInputLayerSize, hidden_size) \n",
    "Wch=np.random.randn(concatHiddenLayerInputLayerSize, hidden_size) \n",
    "Wyh=np.random.randn(hidden_size, vocab_size) \n",
    "\n",
    "mWih = np.zeros_like(Wih)\n",
    "mWfh = np.zeros_like(Wfh)\n",
    "mWoh = np.zeros_like(Woh)\n",
    "mWgh = np.zeros_like(Wgh)\n",
    "mWch = np.zeros_like(Wch)\n",
    "mWyh = np.zeros_like(Wyh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we are using Adagrad so we need to initialize the cache and the eps.\n",
    "\n",
    "![AdaGradImplementation](https://i.imgur.com/p9IvCME.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias\n",
    "mbi = np.zeros_like(bi)\n",
    "mbf = np.zeros_like(bf)\n",
    "mbo = np.zeros_like(bo)\n",
    "mbg = np.zeros_like(bg)\n",
    "mbc = np.zeros_like(bc)\n",
    "mby = np.zeros_like(by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Start\n",
    "\n",
    "Note: this will run forever until you terminate.  Maybe later if I have some time I could add a logging system and maybe a termination loops. For now this is good just to see how well things work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/7OpZrWz.png)\n",
    "![image](https://i.imgur.com/EwjRQ0C.png) \n",
    "![](https://i.imgur.com/jzIcL6x.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:   \n",
    "  \n",
    "  # Step #1 Truncated Backprop through time paradigm\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # Because we are doing truncated backprop thorugh time (by 25 steps) we need to see if we get an overflow.  If so then we need\n",
    "  # to initialize everything back to zero.  this means that the previous hidden state is now 0 and the data pointer is also back\n",
    "  # to zero\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    # reset RNN memory\n",
    "    hprev = np.zeros((hidden_size,1)) \n",
    "    # go from start of data\n",
    "    p = 0 #\n",
    "\n",
    "  # Step #2 - Obtain Inputs\n",
    "  # Get the sequence of inputs in the database with length seq_length\n",
    "  # input is a list of indicies in the char_to_ix\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "\n",
    "  # Step #3 - Obtain Outputs\n",
    "  # Get the sequence of outputs in the database with length seq_length\n",
    "  # basically the next character of the input\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # Step #4 - Backprop and calculate loss\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  \n",
    "  # Step #5 - Parameter updates \n",
    "  # perform parameter update with Adagrad\n",
    "  # This is an efficient way of looping through all the parameters that needs to be update\n",
    "  # zip each group of parameter types using the zip function.  \n",
    "  # So the zip function places zip([parametsr],[derivatives of the parameters],[memory of the paramters])\n",
    "  # loop through them so in this case it will first take the Wxh i.e. the first terms in each of the zipped lists.\n",
    "  # In other words \n",
    "  # param = Wxh \n",
    "  # dparam = dWxh \n",
    "  # mem = mWxh\n",
    "  # then you apply the adagra update rule\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                            [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                            [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  # Step #6 - Randoomly sample what the model is generating\n",
    "  # sample from the model now and then by outputing what it generated and also show the loss\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print '----\\n %s \\n----' % (txt, )\n",
    "    print 'iter %d, loss: %f' % (n, smooth_loss) \n",
    "    \n",
    "\n",
    "  # Step #7 - increment the iteration and also the step sequence through the database.  \n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
