{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('input.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data and calculate indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1681 characters, 37 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions and Derivatives\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
    "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random weights with normal distribution (`0`, `weight_sd`) for $tanh$ activation function and (`0.5`, `weight_sd`) for $sigmoid$ activation function.\n",
    "\n",
    "Biases are initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', \n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = Param('W_i',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_g = Param('W_g',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_g = Param('b_g',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = Param('W_o',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_g, self.W_o, self.W_v,\n",
    "               self.b_f, self.b_i, self.b_g, self.b_o, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "![LSTM](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
    "\n",
    "*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n",
    "\n",
    "#### Concatenation of $h_{t-1}$ and $x_t$\n",
    "\\begin{align}\n",
    "z & = [h_{t-1}, x_t] \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### LSTM functions\n",
    "\\begin{align}\n",
    "f_t & = \\sigma(W_f \\cdot z + b_f) \\\\\n",
    "i_t & = \\sigma(W_i \\cdot z + b_i) \\\\\n",
    "\\bar{C}_t & = tanh(W_C \\cdot z + b_C) \\\\\n",
    "C_t & = f_t * C_{t-1} + i_t * \\bar{C}_t \\\\\n",
    "o_t & = \\sigma(W_o \\cdot z + b_t) \\\\\n",
    "h_t &= o_t * tanh(C_t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Logits\n",
    "\\begin{align}\n",
    "v_t &= W_v \\cdot h_t + b_v \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Softmax\n",
    "\\begin{align}\n",
    "\\hat{y_t} &= \\text{softmax}(v_t)\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "    \n",
    "    z = np.row_stack((h_prev, x))\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    g = tanh(np.dot(p.W_g.v, z) + p.b_g.v)\n",
    "\n",
    "    C = f * C_prev + i * g\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, g, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "#### Loss\n",
    "\n",
    "\\begin{align}\n",
    "L_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\n",
    "L &= L_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Gradients\n",
    "\n",
    "\\begin{align}\n",
    "dv_t &= \\hat{y_t} - y_t \\\\\n",
    "dh_t &= dh'_t + W_y^T \\cdot dv_t \\\\\n",
    "do_t &= dh_t * \\text{tanh}(C_t) \\\\\n",
    "dC_t &= dC'_t + dh_t * o_t * (1 - \\text{tanh}^2(C_t))\\\\\n",
    "d\\bar{C}_t &= dC_t * i_t \\\\\n",
    "di_t &= dC_t * \\bar{C}_t \\\\\n",
    "df_t &= dC_t * C_{t-1} \\\\\n",
    "\\\\\n",
    "df'_t &= f_t * (1 - f_t) * df_t \\\\\n",
    "di'_t &= i_t * (1 - i_t) * di_t \\\\\n",
    "d\\bar{C}'_{t-1} &= (1 - \\bar{C}_t^2) * d\\bar{C}_t \\\\\n",
    "do'_t &= o_t * (1 - o_t) * do_t \\\\\n",
    "dz_t &= W_f^T \\cdot df'_t \\\\\n",
    "     &+ W_i^T \\cdot di_t \\\\\n",
    "     &+ W_C^T \\cdot d\\bar{C}_t \\\\\n",
    "     &+ W_o^T \\cdot do_t \\\\\n",
    "\\\\\n",
    "[dh'_{t-1}, dx_t] &= dz_t \\\\\n",
    "dC'_t &= f_t * dC_t\n",
    "\\end{align}\n",
    "\n",
    "* $dC'_t = \\frac{\\partial L_{t+1}}{\\partial C_t}$ and $dh'_t = \\frac{\\partial L_{t+1}}{\\partial h_t}$\n",
    "* $dC_t = \\frac{\\partial L}{\\partial C_t} = \\frac{\\partial L_t}{\\partial C_t}$ and $dh_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_{t}}{\\partial h_t}$\n",
    "* All other derivatives are of $L$\n",
    "* `target` is target character index $y_t$\n",
    "* `dh_next` is $dh'_{t}$ (size H x 1)\n",
    "* `dC_next` is $dC'_{t}$ (size H x 1)\n",
    "* `C_prev` is $C_{t-1}$ (size H x 1)\n",
    "* $df'_t$, $di'_t$, $d\\bar{C}'_t$, and $do'_t$ are *also* assigned to `df`, `di`, `dC_bar`, and `do` in the **code**.\n",
    "* *Returns* $dh_t$ and $dC_t$\n",
    "\n",
    "#### Model parameter gradients\n",
    "\n",
    "\\begin{align}\n",
    "dW_v &= dv_t \\cdot h_t^T \\\\\n",
    "db_v &= dv_t \\\\\n",
    "\\\\\n",
    "dW_f &= df'_t \\cdot z^T \\\\\n",
    "db_f &= df'_t \\\\\n",
    "\\\\\n",
    "dW_i &= di'_t \\cdot z^T \\\\\n",
    "db_i &= di'_t \\\\\n",
    "\\\\\n",
    "dW_C &= d\\bar{C}'_t \\cdot z^T \\\\\n",
    "db_C &= d\\bar{C}'_t \\\\\n",
    "\\\\\n",
    "dW_o &= do'_t \\cdot z^T \\\\\n",
    "db_o &= do'_t \\\\\n",
    "\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, g, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, g, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dg = dC * i\n",
    "    dg = dtanh(g) * dg\n",
    "    p.W_g.d += np.dot(dg, z.T)\n",
    "    p.b_g.d += dg\n",
    "\n",
    "    di = dC * g\n",
    "    di = dsigmoid(i) * di\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_g.v.T, dg)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear gradients before each backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip gradients to mitigate exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
    "\n",
    "* `input`, `target` are list of integers, with character indexes.\n",
    "* `h_prev` is the array of initial `h` at $h_{-1}$ (size H x 1)\n",
    "* `C_prev` is the array of initial `C` at $C_{-1}$ (size H x 1)\n",
    "* *Returns* loss, final $h_T$ and $C_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "    g_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)\n",
    "    C_s[-1] = np.copy(C_prev)\n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        (z_s[t], f_s[t], i_s[t],\n",
    "        g_s[t], C_s[t], o_s[t], h_s[t],\n",
    "        v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "            \n",
    "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
    "        \n",
    "    clear_gradients()\n",
    "\n",
    "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                     z = z_s[t], f = f_s[t], i = i_s[t], g = g_s[t],\n",
    "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the graph and display a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for 200 letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
    "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delay the keyboard interrupt to prevent the training \n",
    "from stopping in the middle of an iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import signal\n",
    "\n",
    "# class DelayedKeyboardInterrupt(object):\n",
    "#     def __enter__(self):\n",
    "#         self.signal_received = False\n",
    "#         self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "#     def handler(self, sig, frame):\n",
    "#         self.signal_received = (sig, frame)\n",
    "#         print('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "\n",
    "#     def __exit__(self, type, value, traceback):\n",
    "#         signal.signal(signal.SIGINT, self.old_handler)\n",
    "#         if self.signal_received:\n",
    "#             self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     try:\n",
    "#         with DelayedKeyboardInterrupt():\n",
    "#             # Reset\n",
    "#             if pointer + T_steps >= len(data) or iteration == 0:\n",
    "#                 g_h_prev = np.zeros((H_size, 1))\n",
    "#                 g_C_prev = np.zeros((H_size, 1))\n",
    "#                 pointer = 0\n",
    "\n",
    "\n",
    "#             inputs = ([char_to_idx[ch] \n",
    "#                        for ch in data[pointer: pointer + T_steps]])\n",
    "#             targets = ([char_to_idx[ch] \n",
    "#                         for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "#             loss, g_h_prev, g_C_prev = \\\n",
    "#                 forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "#             smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "#             # Print every hundred steps\n",
    "#             if iteration % 100 == 0:\n",
    "#                 update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "#             update_paramters()\n",
    "\n",
    "#             plot_iter = np.append(plot_iter, [iteration])\n",
    "#             plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "#             pointer += T_steps\n",
    "#             iteration += 1\n",
    "#     except KeyboardInterrupt:\n",
    "#         update_status(inputs, g_h_prev, g_C_prev)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " I geen hathays \n",
      "When I leovathind is over \n",
      "And I swear that it's true \n",
      "No buts or maybhs someor aay \n",
      "When I leave you behind \n",
      "It pratnd \n",
      "\n",
      "We m out honey \n",
      "What am I doing without you \n",
      "And all of the th \n",
      "----\n",
      "iter 6600, loss 1.129341\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9b2020035120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                 for ch in data[pointer + 1: pointer + T_steps + 1]])\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_h_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_C_prev\u001b[0m \u001b[1;33m=\u001b[0m         \u001b[0mforward_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_h_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_C_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-a18d6eecc89b>\u001b[0m in \u001b[0;36mforward_backward\u001b[1;34m(inputs, targets, h_prev, C_prev)\u001b[0m\n\u001b[0;32m     36\u001b[0m                      \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                      \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                      y = y_s[t])\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mclip_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-06d3bfa6b6c3>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(target, dh_next, dC_next, C_prev, z, f, i, g, C, o, h, v, y, p)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mdg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdC\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Reset\n",
    "    if pointer + T_steps >= len(data) or iteration == 0:\n",
    "        g_h_prev = np.zeros((H_size, 1))\n",
    "        g_C_prev = np.zeros((H_size, 1))\n",
    "        pointer = 0\n",
    "\n",
    "\n",
    "    inputs = ([char_to_idx[ch] \n",
    "               for ch in data[pointer: pointer + T_steps]])\n",
    "    targets = ([char_to_idx[ch] \n",
    "                for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "    loss, g_h_prev, g_C_prev = \\\n",
    "        forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # Print every hundred steps\n",
    "    if iteration % 100 == 0:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "    update_paramters()\n",
    "\n",
    "    plot_iter = np.append(plot_iter, [iteration])\n",
    "    plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "    pointer += T_steps\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
