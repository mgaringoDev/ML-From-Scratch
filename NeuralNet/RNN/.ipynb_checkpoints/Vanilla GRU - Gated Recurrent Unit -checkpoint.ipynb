{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the following resources:\n",
    "- Background\n",
    "    - [Emipircal Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555.pdf)\n",
    "    - [Introduction to Sequence Models — RNN, Bidirectional RNN, LSTM, GRU](https://towardsdatascience.com/introduction-to-sequence-models-rnn-bidirectional-rnn-lstm-gru-73927ec9df15)\n",
    "    - [Recurrent Neural Networks Introduction](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/recurrent_neural_networks.html)    \n",
    "- Standford\n",
    "    - [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)    \n",
    "    - [Vector,Matrix and Tensor Derivatives Cheatsheet](http://cs231n.stanford.edu/vecDerivs.pdf)\n",
    "- Vanilla\n",
    "    - [Rnn (vanial, GRU and LSTM) from scratch](https://github.com/ankitesh97/NewsGenerator/blob/master/Gru.py)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data to be learnt from is in the same directory as this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('input.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data and calculate indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 2421 characters, 42 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8 # adagrad initialization note that this is usually between 1e-4 and 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))\n",
    "\n",
    "# Number of characters for the model to generate\n",
    "numOfCharToGenerate = 100\n",
    "\n",
    "# Number of iteration before showing the progress of the model\n",
    "numOfIterToShowProgress = 200\n",
    "\n",
    "# Number of iterations to use for training\n",
    "numItr = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are initializing using the [Xavier initalization method](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi). Which says that we multiply a random distribution with either\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}(W) = \\frac{1}{n_\\text{in}}\\\\\n",
    "\\text{Var}(W) = \\frac{2}{n_\\text{in} + n_\\text{out}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "In short, it helps signals reach deep into the network.\n",
    "\n",
    "- If the weights in a network start too small, then the signal shrinks as it passes through each layer until it’s too tiny to be useful.\n",
    "- If the weights in a network start too large, then the signal grows as it passes through each layer until it’s too massive to be useful.\n",
    "\n",
    "Xavier initialization makes sure the weights are ‘just right’, keeping the signal in a reasonable range of values through many layers. To go any further than this, you’re going to need a small amount of statistics - specifically you need to know about random distributions and their variance.\n",
    "\n",
    "For a detailed explanation refer to this [blog](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) if the paper is too convoluted. We use the second implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        # update gate\n",
    "        self.W_u = Param('W_u', \n",
    "                         np.random.randn(H_size, z_size) * np.sqrt(2.0/(H_size+z_size)))\n",
    "        self.b_u = Param('b_u',\n",
    "                         np.zeros((H_size, 1)))\n",
    "        \n",
    "        # reset gate\n",
    "        self.W_r = Param('W_r',\n",
    "                         np.random.randn(H_size, z_size) * np.sqrt(2.0/(H_size+z_size)))\n",
    "        self.b_r = Param('b_r',\n",
    "                         np.zeros((H_size, 1)))\n",
    "        \n",
    "         # memory gate\n",
    "        self.W_hbar = Param('W_har',\n",
    "                         np.random.randn(H_size, z_size) * np.sqrt(2.0/(H_size+z_size)))\n",
    "        self.b_hbar = Param('b_har',\n",
    "                         np.zeros((H_size, 1)))        \n",
    "        \n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * np.sqrt(2.0/(X_size+H_size)))\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    # this method makes it easier to loop through each paramters for either updating or clearing them out for initialization.    \n",
    "    def all(self):\n",
    "        return [self.W_u, self.W_r, self.W_hbar, self.W_v,\n",
    "               self.b_u, self.b_r, self.b_hbar, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions and Derivatives\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
    "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "![GRU](https://i.imgur.com/HYMgdg9.png)\n",
    "\n",
    "*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n",
    "and since we have two zeds I will associate the $z_t$ with $u_t$ for update state.\n",
    "\n",
    "### Calculations\n",
    "#### Concatenation of hprev and xt\n",
    "\\begin{align}\n",
    "z & = [h_{t-1}, x_t] \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### LSTM functions\n",
    "\\begin{align}\n",
    "u_t & = \\sigma(W_u \\cdot z + b_u) \\\\\n",
    "r_t & = \\sigma(W_r \\cdot z + b_i) \\\\\n",
    "\\bar h_t & = tanh(W_{\\bar h} \\cdot [r_t * h_{t-1}, x_t])\\\\\n",
    "h_t &= (1 - u_t) * h_{t-1} + u_t * \\bar h_t \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Logits\n",
    "\\begin{align}\n",
    "v_t &= W_v \\cdot h_t + b_v \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Softmax\n",
    "\\begin{align}\n",
    "\\hat{y_t} &= \\text{softmax}(v_t)\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just go through the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)    \n",
    "    \n",
    "    # concatontae hiddenstate matrix with the input matrix to make calculatations cleaner\n",
    "    z = np.row_stack((h_prev, x))\n",
    "    \n",
    "    # calculate LSTM gates \n",
    "    u = sigmoid(np.dot(p.W_u.v, z) + p.b_u.v)\n",
    "    r = sigmoid(np.dot(p.W_r.v, z) + p.b_r.v)\n",
    "    \n",
    "    z_prime = np.row_stack((r*h_prev, x))\n",
    "    hbar = tanh(np.dot(p.W_hbar.v, z_prime) + p.b_hbar.v)\n",
    "        \n",
    "    \n",
    "    # hidden state for this tick    \n",
    "    h = (1-u)*h_prev + u*hbar\n",
    "    \n",
    "    # softmax loss\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, u, r, z_prime, hbar, h, v, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "#### Loss\n",
    "\n",
    "\\begin{align}\n",
    "L_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\n",
    "L &= L_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Gradients\n",
    "Follow the gradient backwards through the computational graphs.  Same as the LSTM.\n",
    "\n",
    "The most important thing to note here is the junction cases.  At these points add all the in coming gradients together before going upstream.  Apart from that there is nothing really tricky about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward()\n",
    "\n",
    "Note: When traversing through the computational network to get the local gradient of a node it is [**upstream**] * [**downstream**]\n",
    "\n",
    "- **add gate:** gradient distributer ... pass the exact same value through each of the branches\n",
    "- **max gate:** gradient router ... passes the entire upstream gradient to the branch with the highest upstream value\n",
    "- **mul gate:** gradient switcher ... local gradient is the value of the other branch (think of this a s ad/dx of a linear function)\n",
    "- **functional gate:** gradient of the function ... take the derivative of this function with respect to the upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, h_prev,\n",
    "             z, u, r, z_prime, hbar, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, u, r, hbar, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "    \n",
    "    # Loss function\n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "    \n",
    "    # Loss function wrt logit\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "    \n",
    "    # logit wrt hidden state\n",
    "    # NOTE: from here on dh is common in the calculations for all the weights\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    \n",
    "    \n",
    "    # calculate the affects of the ---------------------  update gate    \n",
    "    # backprop throught he add gate which is just a distributer\n",
    "    # backprop through the -1 which just multiplies dh by -1\n",
    "    # add hbar because there is a junction \n",
    "    du = dh * -1 * h_prev + hbar\n",
    "    # backprop through the sigmoid gate\n",
    "    du = dsigmoid(u) * du\n",
    "    # backprop through the final mul gate\n",
    "    p.W_u.d += np.dot(du, z.T)\n",
    "    p.b_u.d += du   \n",
    "    \n",
    "    # calculate the affects of the ---------------------  reset gate    \n",
    "    # backprop through the mul gate\n",
    "    # add hprev because of the junction\n",
    "    dr = dh * (1 - u) + h_prev\n",
    "    # bcakprop through the mul gate\n",
    "    dr = dr * z_prime[:H_size, :]\n",
    "    # backprop through the sigmoid\n",
    "    dr = dsigmoid(r) * dr\n",
    "    # backpropr through the final mul gate\n",
    "    p.W_r.d += np.dot(dr, z.T)\n",
    "    p.b_r.d += dr   \n",
    "    \n",
    "    # calculate the affects of the ---------------------  memory gate    \n",
    "    # backprop through the mul gate\n",
    "    dhbar = dh * u\n",
    "    # backprop through the tanh gate\n",
    "    dhbar = dtanh(hbar) * dhbar\n",
    "    # backprop through the final mul gate but with z_prime\n",
    "    dhbar = dhbar + z_prime[:H_size, :]\n",
    "    # backpropr through the final mul gat but with \n",
    "    p.W_hbar.d += np.dot(dhbar, z.T)\n",
    "    p.b_hbar.d += dhbar    \n",
    "    \n",
    "    \n",
    "    # calculate the affects of the ---------------------- z weigths\n",
    "    dz = (np.dot(p.W_u.v.T, du)\n",
    "         + np.dot(p.W_r.v.T, dr)         \n",
    "         + np.dot(p.W_hbar.v.T, dhbar))\n",
    "    \n",
    "    # only take the portion of the input that is affected by the hidden states.  Recall that z is a concatination of the\n",
    "    # hidden state and the input one hot vector\n",
    "    dh_prev = dz[:H_size, :]\n",
    "       \n",
    "    \n",
    "    return dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear gradients \n",
    "Used for initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip gradients \n",
    "This is to mitigate exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    # clip the crapids to [-1,1]\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -5, 5, out=p.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-Backward\n",
    "\n",
    "Perform forward and then back prop in one function to keep things nice and tidy when reading the main LSTM loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, u_s, r_s,  = {}, {}, {}, {}\n",
    "    z_prime_s = {}\n",
    "    hbar_s, h_s = {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)    \n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        \n",
    "        # one hot encode the input vector\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        # Forward pass\n",
    "        (z_s[t], u_s[t], r_s[t],\n",
    "        z_prime_s[t], hbar_s[t], \n",
    "         h_s[t], v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1],parameters) \n",
    "        \n",
    "        # Loss for this tick\n",
    "        loss += -np.log(y_s[t][targets[t], 0]) \n",
    "    \n",
    "    # clear the gradients for the next step size\n",
    "    clear_gradients()\n",
    "    \n",
    "    #dh from the next character\n",
    "    dh_next = np.zeros_like(h_s[0])      \n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        \n",
    "        dh_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     h_prev = h_s[t], \n",
    "                     z = z_s[t], u = u_s[t], r = r_s[t], z_prime = z_prime_s[t],\n",
    "                     hbar = hbar_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "    \n",
    "    # clip the gradients just in case they blow up\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the next character\n",
    "Used to see what the model is generating at some random sample point in the ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h_prev,first_char_idx, sentence_length):\n",
    "    \n",
    "    # one hot vector for the seed\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    # obtain current c state and hidden state\n",
    "    h = h_prev\n",
    "        \n",
    "    # running container for the generated characters\n",
    "    indexes = []\n",
    "    \n",
    "    # run model for sentence_length number of ticks\n",
    "    for t in range(sentence_length):\n",
    "        \n",
    "        # perform forward pass ... note that we only care about th enew C state and hidden state as well as the output\n",
    "        # probabilities because we are not performing backprop\n",
    "        _, _, _, _, _, h, _, p = forward(x, h)        \n",
    "        \n",
    "        # select a random with some probability associated with values\n",
    "        # np.random.choice(allPossibileOutcomes, probabilityOfTheOutcomes)\n",
    "        # choose a number in the vocab based on the probabilty distribution found in previous step\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        \n",
    "        # one-hot encode this in a vector\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # append running vector outputs\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the graph and display a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, numChar):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for numOfCharToGenerate letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, inputs[0], numChar)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameters\n",
    "\n",
    "We will use adam descibed below.  \n",
    "\n",
    "![AdaGradImplementation](https://i.imgur.com/p9IvCME.png)\n",
    "\n",
    "Here eps and learning_rate is a hyper parameter and dx is the current gradient for a particular weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " innwyhinn'te u onoe owmevae  wnyTi'h osoravsenllI'lings wwebthele   se onne  yyh  o  Bkesae ool lero \n",
      "----\n",
      "iter 5000, loss 67.059696\n"
     ]
    }
   ],
   "source": [
    "while iteration<(numItr+1):\n",
    "    # Reset\n",
    "    \n",
    "    # if the current pointer is outisde the number data points we have +  the step size or this is the first run of the loop\n",
    "    # initialize the hidden state and c state to zero and make the pointer point to the first character in our\n",
    "    # dataset\n",
    "    if pointer + T_steps >= len(data) or iteration == 0:\n",
    "        g_h_prev = np.zeros((H_size, 1))\n",
    "        g_C_prev = np.zeros((H_size, 1))\n",
    "        pointer = 0\n",
    "\n",
    "    # obtain the T_steps number of character of the input and target values\n",
    "    inputs = ([char_to_idx[ch] \n",
    "               for ch in data[pointer: pointer + T_steps]])\n",
    "    targets = ([char_to_idx[ch] \n",
    "                for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "    \n",
    "    # perform back and forwad pass throught through the LSTM with T-steps number of ticks\n",
    "    loss, g_h_prev = \\\n",
    "        forward_backward(inputs, targets, g_h_prev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # Print every hundred steps\n",
    "    if iteration % numOfIterToShowProgress == 0:\n",
    "        update_status(inputs, g_h_prev, numOfCharToGenerate)\n",
    "    \n",
    "    # update the parameters\n",
    "    update_paramters()\n",
    "\n",
    "    # for visulaization purposes\n",
    "    plot_iter = np.append(plot_iter, [iteration])\n",
    "    plot_loss = np.append(plot_loss, [loss])\n",
    "    \n",
    "    # increment the datapointer to point to the next T-step as well as the iteration count\n",
    "    pointer += T_steps\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
