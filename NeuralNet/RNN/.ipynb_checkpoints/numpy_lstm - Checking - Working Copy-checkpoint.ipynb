{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('input.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data and calculate indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1681 characters, 37 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions and Derivatives\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
    "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random weights with normal distribution (`0`, `weight_sd`) for $tanh$ activation function and (`0.5`, `weight_sd`) for $sigmoid$ activation function.\n",
    "\n",
    "Biases are initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', \n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = Param('W_i',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_C = Param('W_C',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_C = Param('b_C',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = Param('W_o',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
    "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "![LSTM](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
    "\n",
    "*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n",
    "\n",
    "#### Concatenation of $h_{t-1}$ and $x_t$\n",
    "\\begin{align}\n",
    "z & = [h_{t-1}, x_t] \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### LSTM functions\n",
    "\\begin{align}\n",
    "f_t & = \\sigma(W_f \\cdot z + b_f) \\\\\n",
    "i_t & = \\sigma(W_i \\cdot z + b_i) \\\\\n",
    "\\bar{C}_t & = tanh(W_C \\cdot z + b_C) \\\\\n",
    "C_t & = f_t * C_{t-1} + i_t * \\bar{C}_t \\\\\n",
    "o_t & = \\sigma(W_o \\cdot z + b_t) \\\\\n",
    "h_t &= o_t * tanh(C_t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Logits\n",
    "\\begin{align}\n",
    "v_t &= W_v \\cdot h_t + b_v \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Softmax\n",
    "\\begin{align}\n",
    "\\hat{y_t} &= \\text{softmax}(v_t)\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "    \n",
    "    z = np.row_stack((h_prev, x))\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "#### Loss\n",
    "\n",
    "\\begin{align}\n",
    "L_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\n",
    "L &= L_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Gradients\n",
    "\n",
    "\\begin{align}\n",
    "dv_t &= \\hat{y_t} - y_t \\\\\n",
    "dh_t &= dh'_t + W_y^T \\cdot dv_t \\\\\n",
    "do_t &= dh_t * \\text{tanh}(C_t) \\\\\n",
    "dC_t &= dC'_t + dh_t * o_t * (1 - \\text{tanh}^2(C_t))\\\\\n",
    "d\\bar{C}_t &= dC_t * i_t \\\\\n",
    "di_t &= dC_t * \\bar{C}_t \\\\\n",
    "df_t &= dC_t * C_{t-1} \\\\\n",
    "\\\\\n",
    "df'_t &= f_t * (1 - f_t) * df_t \\\\\n",
    "di'_t &= i_t * (1 - i_t) * di_t \\\\\n",
    "d\\bar{C}'_{t-1} &= (1 - \\bar{C}_t^2) * d\\bar{C}_t \\\\\n",
    "do'_t &= o_t * (1 - o_t) * do_t \\\\\n",
    "dz_t &= W_f^T \\cdot df'_t \\\\\n",
    "     &+ W_i^T \\cdot di_t \\\\\n",
    "     &+ W_C^T \\cdot d\\bar{C}_t \\\\\n",
    "     &+ W_o^T \\cdot do_t \\\\\n",
    "\\\\\n",
    "[dh'_{t-1}, dx_t] &= dz_t \\\\\n",
    "dC'_t &= f_t * dC_t\n",
    "\\end{align}\n",
    "\n",
    "* $dC'_t = \\frac{\\partial L_{t+1}}{\\partial C_t}$ and $dh'_t = \\frac{\\partial L_{t+1}}{\\partial h_t}$\n",
    "* $dC_t = \\frac{\\partial L}{\\partial C_t} = \\frac{\\partial L_t}{\\partial C_t}$ and $dh_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_{t}}{\\partial h_t}$\n",
    "* All other derivatives are of $L$\n",
    "* `target` is target character index $y_t$\n",
    "* `dh_next` is $dh'_{t}$ (size H x 1)\n",
    "* `dC_next` is $dC'_{t}$ (size H x 1)\n",
    "* `C_prev` is $C_{t-1}$ (size H x 1)\n",
    "* $df'_t$, $di'_t$, $d\\bar{C}'_t$, and $do'_t$ are *also* assigned to `df`, `di`, `dC_bar`, and `do` in the **code**.\n",
    "* *Returns* $dh_t$ and $dC_t$\n",
    "\n",
    "#### Model parameter gradients\n",
    "\n",
    "\\begin{align}\n",
    "dW_v &= dv_t \\cdot h_t^T \\\\\n",
    "db_v &= dv_t \\\\\n",
    "\\\\\n",
    "dW_f &= df'_t \\cdot z^T \\\\\n",
    "db_f &= df'_t \\\\\n",
    "\\\\\n",
    "dW_i &= di'_t \\cdot z^T \\\\\n",
    "db_i &= di'_t \\\\\n",
    "\\\\\n",
    "dW_C &= d\\bar{C}'_t \\cdot z^T \\\\\n",
    "db_C &= d\\bar{C}'_t \\\\\n",
    "\\\\\n",
    "dW_o &= do'_t \\cdot z^T \\\\\n",
    "db_o &= do'_t \\\\\n",
    "\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    p.W_C.d += np.dot(dC_bar, z.T)\n",
    "    p.b_C.d += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_C.v.T, dC_bar)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear gradients before each backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip gradients to mitigate exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
    "\n",
    "* `input`, `target` are list of integers, with character indexes.\n",
    "* `h_prev` is the array of initial `h` at $h_{-1}$ (size H x 1)\n",
    "* `C_prev` is the array of initial `C` at $C_{-1}$ (size H x 1)\n",
    "* *Returns* loss, final $h_T$ and $C_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)\n",
    "    C_s[-1] = np.copy(C_prev)\n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        (z_s[t], f_s[t], i_s[t],\n",
    "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
    "        v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "            \n",
    "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
    "        \n",
    "    clear_gradients()\n",
    "\n",
    "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
    "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the graph and display a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for 200 letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
    "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delay the keyboard interrupt to prevent the training \n",
    "from stopping in the middle of an iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import signal\n",
    "\n",
    "# class DelayedKeyboardInterrupt(object):\n",
    "#     def __enter__(self):\n",
    "#         self.signal_received = False\n",
    "#         self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "#     def handler(self, sig, frame):\n",
    "#         self.signal_received = (sig, frame)\n",
    "#         print('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "\n",
    "#     def __exit__(self, type, value, traceback):\n",
    "#         signal.signal(signal.SIGINT, self.old_handler)\n",
    "#         if self.signal_received:\n",
    "#             self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     try:\n",
    "#         with DelayedKeyboardInterrupt():\n",
    "#             # Reset\n",
    "#             if pointer + T_steps >= len(data) or iteration == 0:\n",
    "#                 g_h_prev = np.zeros((H_size, 1))\n",
    "#                 g_C_prev = np.zeros((H_size, 1))\n",
    "#                 pointer = 0\n",
    "\n",
    "\n",
    "#             inputs = ([char_to_idx[ch] \n",
    "#                        for ch in data[pointer: pointer + T_steps]])\n",
    "#             targets = ([char_to_idx[ch] \n",
    "#                         for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "#             loss, g_h_prev, g_C_prev = \\\n",
    "#                 forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "#             smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "#             # Print every hundred steps\n",
    "#             if iteration % 100 == 0:\n",
    "#                 update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "#             update_paramters()\n",
    "\n",
    "#             plot_iter = np.append(plot_iter, [iteration])\n",
    "#             plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "#             pointer += T_steps\n",
    "#             iteration += 1\n",
    "#     except KeyboardInterrupt:\n",
    "#         update_status(inputs, g_h_prev, g_C_prev)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD0CAYAAABtjRZ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmYVNWB/vFvLywijYqyiqRF5YgbARdcAHVwiWDiEuMkMTHRZDJOnBk1RsMQCRjRYKK4REXjxi8EMyoMbsiq7CC7ICCn2aGRptl6p5fqqt8fVbf6Vvet7qapXm7xfp7H57l1762qc6n2rVPnniUlFAohIiL+lNrcBRARkYZTiIuI+JhCXETExxTiIiI+phAXEfGx9KZ6I2NMG+BSYC9Q2VTvKyLic2lAN2CFtbas+sEmC3HCAb6wCd9PRCSZDAIWVd/ZlCG+F2DSpEl07dq1Cd9WRMS/cnJyuOuuuyCSodU1ZYhXAnTt2pUePXo04duKiCQFz2Zo3dgUEfExhbiIiI/V2pxijGkFvAVkAm2AMUA28DGwOXLaeGvtu8aYUcAwIAA8aK1d3liFFhGRsLraxH8CHLTW/tQYcyqwBvgjMM5a+6xzkjGmP3A1MAA4A5hCuDeKiIg0orpC/H1gsutxALgYMMaYWwjXxh8EBgKzrLUhYJcxJt0Y08lau78xCi0iImG1tolba4ustYXGmAzCYf4YsBx4xFo7GNgGjAI6APmupxYCJzVOkUVExFHnjU1jzBnAXGCitfYdYKq1dlXk8FSgH1AAZLielgHkJbKgmcOn8cKczXWfKCJyHKk1xI0xXYBZwO+stW9Fds80xlwW2R4CrAIWAzcaY1KNMT2BVGvtgUQX9rk5WYl+SRERX6urTXwEcAow0hgzMrLvN8DzxphyIAf4lbW2wBizEFhK+Ivh/sYqsIiIVKk1xK21DwAPeBy60uPc0cDohJRKRETqRYN9RER8TCEuIuJjCnERER9TiIuI+JhCXETExxTiIiI+phAXEfExhbiIiI8pxEVEfEwhLiLiYwpxEREfU4iLiPiYQlxExMcU4iIiPqYQFxHxMYW4iIiPKcRFRHxMIS4i4mMKcRERH1OIi4j4mEJcRMTHFOIiIj6mEBcR8TGFuIiIjynERUR8TCEuIuJjCnERER/zRYiHQqHmLoKISIvkixAXERFvCnERER9TiIuI+JgvQlxN4iIi3tJrO2iMaQW8BWQCbYAxwEZgAhAC1gP3W2uDxphRwDAgADxorV3eeMUWERGouyb+E+CgtXYQcBPwEjAOeCyyLwW4xRjTH7gaGAD8EHi58YosIiKOukL8fWCk63EAuBiYH3k8HbgOGAjMstaGrLW7gHRjTKdEF1ZERGLVGuLW2iJrbaExJgOYDDwGpFhrnVbqQuAkoAOQ73qqs19ERBpRnTc2jTFnAHOBidbad4Cg63AGkAcURLar708I3dcUEfFWa4gbY7oAs4DfWWvfiuxeY4y5JrJ9E7AQWAzcaIxJNcb0BFKttQcaqcwiIhJRa+8UYARwCjDSGOO0jT8AvGiMaQ18DUy21lYaYxYCSwl/MdzfWAUWEZEqtYa4tfYBwqFd3dUe544GRiekVCIiUi8+GeyjVnERES++CHEREfGmEBcR8TGFuIiIjynERUR8zBchrtuaIiLefBHiIiLiTSEuIuJjCnERER/zRYhrrI+IiDdfhLiIiHhTiIuI+JhCXETEx3wR4iH1FBcR8eSLEBcREW8KcRERH1OIi4j4mEJcRMTHfBHiGuwjIuLNFyEuIiLeFOIiIj6mEBcR8TGFuIiIjynERUR8TCEuIuJjCnERER9TiIuI+JgvQlyDfUREvPkixEVExJtCXETExxTiIiI+5osQ18o+IiLe0utzkjFmAPC0tfYaY0x/4GNgc+TweGvtu8aYUcAwIAA8aK1d3iglFhGRqDpD3BjzKPBToDiyqz8wzlr7rOuc/sDVwADgDGAKcGnCSysiIjHq05yyFbjd9fhiYJgxZoEx5k1jTAYwEJhlrQ1Za3cB6caYTo1QXhERcakzxK21U4AK167lwCPW2sHANmAU0AHId51TCJyUqEKqn7iIiLeG3Nicaq1d5WwD/YACIMN1TgaQd4xlExGROjQkxGcaYy6LbA8BVgGLgRuNManGmJ5AqrX2QKIKKSIi3urVO6Wa/wBeMsaUAznAr6y1BcaYhcBSwl8M9yewjCIiEke9QtxauwO4PLK9GrjS45zRwOjEFU1EROrik8E+IiLixRchLiIi3hTiIiI+phAXEfExX4R4SKN9REQ8+SLERUTEm0JcRMTHFOIiIj7mixBXi7iIiDdfhLiIiHhTiIuI+JhCXETExxTiIiI+5osQ11gfERFvvghxERHxphAXEfExhbiIiI/5I8TVJi4i4skfIS4iIp4U4iIiPqYQFxHxMYW4iIiP+SLEQ7qzKSLiyRchLiIi3hTiIiI+phAXEfExX4S4JsASEfHmixAXERFvCnERER9TiIuI+JgvQlxN4iIi3nwR4iIi4i29PicZYwYAT1trrzHGnA1MIFxBXg/cb60NGmNGAcOAAPCgtXZ5I5VZREQi6qyJG2MeBd4A2kZ2jQMes9YOAlKAW4wx/YGrgQHAD4GXG6e4IiLiVp/mlK3A7a7HFwPzI9vTgeuAgcAsa23IWrsLSDfGdEpoSUVEpIY6Q9xaOwWocO1KsdY69xoLgZOADkC+6xxnf0KENNpHRMRTQ25sBl3bGUAeUBDZrr5fREQaUUNCfI0x5prI9k3AQmAxcKMxJtUY0xNItdYeSFAZRUQkjnr1TqnmYeB1Y0xr4GtgsrW20hizEFhK+Ivh/gSWUURE4qhXiFtrdwCXR7azCPdEqX7OaGB04opWRS3iIiLeNNhHRMTHFOIiIj7mixA/WFTe3EUQEWmRfBHie/OPNHcRRERaJF+EeKs0XxRTRKTJ+SId01JTotvFZYFmLImISMviixB318Qf/3hDM5ZERKRl8UmIV9XED+gmp4hIlC9CvGuHttHtoCbDEhGJ8kWId44J8WYsiIhIC+OLEHfTtLQiIlV8F+JqThERqeK7EC8qDVCpNhUREcCHIb42O5+zRnzKlFXZzV0UEZFm57sQdzz8/lo2fJNf94kiIknMtyEOsCW3KOZxSblGc4rI8cXXIV5WESS3sJTyQJDpX+3lvD/MZP0e1c5F5PjRkOXZmsW3zziZL3fHrr386vytPDplHd/r2512rdMA+GpPPhecflJzFFFEpMn5uia+7UAxADM25JAamSSres+Vg0Vl6lsuIknLNyGe0Tb+j4Y2aamkpYRDPBgKsXrXYUorKtmSW8TFY+bwjy92EqgMcrCorKmKKyLSJHwT4s/e2ZebL+rGv15yRo1jbVqlkn24BIA9h49w+ytLOHfkDLZHaurz7H5GfbSBi8fM4Uh5ZZOWW0SkMfkmxDtntOWlH/fn6TsuqnGsdVoqc+1+APKPVET3O80owVCIGetzACiO04Pl/ZW7+Z//+yrRxRYRaVS+CXG3y3t1jHmc7ppv3N0m7mwGQ0TbzIPBENsPFLNm1+GY13hk8jr+uXxXI5VYRKRx+DLE/+emPjGPSyuqmkg2u/qOu2vizuJAlaEQ1z4zj9teWXJU77nxmwKy9hU2sMQiIo3DlyFeva9JbmHVDUt3N8SqmnjIdeOzYe859MWF3PDcgrjH59lcdh4sjnt8S24h5YFgw95cRCQOX4Z4m/T6Fdtp/w4GISWlqjmlMfz87RVc/Zd5nsf2F5Zx3bgFjPxgfaO8t4gcv3wZ4n26dajXeSMiNyo35RREF1uuzwyIj05ey7hZtuEFrKYosrjzsu0HE/aaIiLg0xCvr0AksAtKA+w6FO6CmFNQGj2eV1LO6I82UBaI7Xb43spsXvx8S8LK4awQqhl0RSTRfBvif/1Rv3qf6659v7tid3T7mVmWCUt28MGaPUf13u+t3M3mODc5i8oC7Mk7ErMvNdKUE6rRmi8icmx8G+Lf7dudqb++8qifN9UV2E64lhzlAKBHJ6/jhue9b3Le/spirhr7ecy+yNsQ1H1NEUkw34Y4QL+epxzT850pVcqOoteI020x3nQsWfuKauxzQjzeHC4VlUFenrslpqukiEh9+DrEAUbefF6Dnzvxi51AeEpbL+HpbWfEhGugAQ3bzk3VeE/95/Jd/GWm5dX5W4/6tUXk+NbgqWiNMWsAZ/Lu7cBrwAtAAJhlrX382ItXt9Patz7m13huTlZ0292X+7k5WZSUV7J1f1XtuqKy6nhZoJLyQJCMtq1qvGZFZZBWkZGkKVRNzuXF+ZIoKtWiFiJydBpUEzfGtAWw1l4T+e8e4FXgx8BAYIAxpn/iitl0nvhkY3TbaRpxZ6875O98dSkXjp7l+TpPT99UY1+8OnzqMQ5EEpHjV0Nr4n2BdsaYWZHXGA20sdZuBTDGzASGAKsTUcjaJHqqcKeJJZ5yV018bXb8VYRWueZmcXqlxGsTT02pvaYuIhJPQ9vES4BngBuB+4C3I/schUCTLK/TFN324tXEa5Pi2naeHwrB3vwj3PLyYva7pgpw5nUJhkKEQiEClfHfo6C0gh0H4g/vF5HjS0NDPAv4h7U2ZK3NItw27p5aMAPI83xmgjVF5TXvSHl0e/iUo5+u1iliMBRiwpIdrN2dx/urqvqrV934DPG3Bds4+/fTyS+p8HgluOWlxVzzzLyjLoOIJKeGhvi9wLMAxpjuQDug2BhzljEmhXANfWFiili7SzPD3x1/uv3CRnuPn765PLq9aMuBej9v6daDrNhxyDWbYtVNTggPQgoGQ1XzuoTg3ZXhcN9fVDWydPehEkZM/YpAZTC60IWICDS8TfxNYIIxZhHhiua9QBCYBKQR7p2yLDFFrN0ZHduxY+wwAOZuymXWxn1N8bZ1CgE/ev0LABY+em14n+tnQygEZ434lO+c35XBvTsB1Sfnqgr737z3JSt2HObWb5/u+V77C8vYebCYSzI7MuzFhRSWBlgQeU8RSW4NCnFrbTnhnijVXX5sxTk2zXVj0KuN2msZuFCIaF/w7MPhofkzNuRw7bmREA+FPLuw1NVF8daXF7Mn7wg7xg5jwzcFDboGEfEn3w/2cWuuLnpebdSbcqrmVnHmO3eH8IZvqnq2pHh0MUxx3RlNjXxK8abRrT5Xi4gcP5IsxMMh99B1vY/5tTLaNHgcVA3fHx9eRcidwe5KdaprrnNn99bcIoY8O4/DxeVV0+iqC6KIVJNUIe7MVnh+9/rNN16bojgLKh8Ld03cPf2ts0RoMBSK3rh84bPNbN1fzAufbY6GfH3mQveyN/9ItMknr6Tcs6lHRPwpqULcCck2rY79suqq9Dq146PhnmjLaRMH+F2k26I7o3ceDHe7n7BkR/S93GVauHk/P379i5gmlniDia740+fRJp9v/3E2t7y86KjL7li+/VCjrY4kIkcvqUI8PdJ43JCAPVod2h5bc4t7+ltnAJG7uSQ9reoavGriv560miVbD1Lomm/Fna0Tv9jJQ+9+6fneXjMt1seizQe487WlvLZgW4OeLyKJl1Qh/uc7LuJXg3sx4MxTG/29GjKbYV0qXDV1d2BHQ9xd03Y2Xd9X7sm5Rn6wPmbu9OqWbD3Aih2Hauyfuiabfa7VjwAOFJVRHgiyNz/862FzrveCGCLS9JIqxLt0aMuIoX1iauKJaB/3crQLSdSHe14Wd7v1nK/Dfd8Xba450Mjdi+XFzzbX+71+/PoyfvDqUgDOHTmdu99aTv6RCh56dy0/e2s509btja5edMmYOXFr9SJ+9uGXezhcXF73iS1YUoW4l16d2jfK6zb0JmNt5tn90W2vmn5hadVQ/MLI4stLt1YtvuysI3q0SiuCLMjaH52zJbewjPvfWc31zy2ItrNP+2pvtCukVpmTZJB9uIQH/vdL7n+n0efpa1RJG+JndDwBSK6ZAT/48psa+/594qrodl1XWtcNSae5xn1Lwf3P5+wOAROX7iC3MLbZxbElt5DM4dNi+sKLtDROR4OcfO+/Y79I2hD/6P6BTH9g0HFVa5y2bm+Nfe5fDOW1zI4IEKgMn5viaqNxfwk6u7cfKGbkhxu4f5J3DWb6VzkAfPpVzfKE3yeIzVG7ukgiJG2In3Jia/p06xCdqvaVu6rWqHjn3wY0V7Ga3DybG92u8AjxvJLyGsdjauIer+n0psmrNtNiKBTicHF5tEaflur95/Wn6Zu48fkF7DyoybxEjlXShrjjnqvOBOCSzKpFlZ2ZD48H42Z7Lz3nOFTsFeK118SD0WaX8I71e/KpDIZ4Z/ku+j0xO1rLTkvx7uq5fHu4V0z1LwGR5uD3H+tJH+KXZnZkx9hhdM5oG92X3gT9yFsK94RY76/KrnHcPQCpojI2nKF6m3hsf/WUlPAcMDf/dREvzMnis6/DtX5nTVJ3X3eAorIA+wpKo8+P15//yWkbyRw+rX4XmCDBYIjPN+2LO2BKkk+ypEDSh7jbd87vCsS2+R5Pxnqs+/mBqy95TqR/eF3/PE5NPCUlhaxIN8Rl2w9Fe9Q4YV89pL/710UMeOqzOkP89YXb67qUhJu0bCf3TljJhx43jyU5JcvXdeJmefKBF3/Uj+IyrSjv5h59ec/bK2ocdzen/GWmBWLHGT307loAissD0VB2vgSqN6c488IEguHaf/VfRGM+2ci6Pc3To2V3ZBqEnAJ/91SQo+f3Kt1xVRNvnZ7KKSe2BmDafw9k6q+v5NTIY7eP/3NgUxetRYkZGOradqa8bRW5Yblxb1VTTXFZZY2+7ampKWw/UMyNzy2IaXt3auKp1UL8jUXbo+3lQNwl6r7z/AIyh087qjlcKiqD/Pb9tXFXRnKaUeL9D11YWsHcTblxjoqf+b1GflyFuNv53U+iX89TeO++K2ocu7BHk6zx3GK55yf/5/JdNY5Xb+sGKC4LRAcLOSGdnprCk9O+xu4rZOaGnOi57husBaUVfLHtIF4+WruHUChUo1eNM1f7oZJyRn+0gac+/TrmeKAyGA34zOHTeOyDr1iXncfkVdn85j3vkafOl1W8pqSH31vLPRNWsLuBA6qk5fF7Ddxx3Ia446xO7fnif4aw4vfXJew1h5zbOWGv1dzGTPu6xj6nP7lbMBRi9a7w2tjOzdLHP94QnTLA3V/9SEV4SoFQKMR9E1fxw799QUFpzVp3bmEZk5bt4pzfT/cckBGoDC88/bdqE3Kd/fvpfP/VJdHH//hiV3RysHg9ZpzSpcY5vi1Sg88pKOWpT7/27K4p/uL3GrjjuA9xgK4ntaVTRpt6n397f++1Lh3FjTAXeSKd1r72a62r806Rx30Fd+3aCWl3a4c7xA9HmkmmrN7DskjziVf3x2AoFL3x+t7K3Vz65JyY964tSNdEvlCir+XqUfNN3pGY+dyd96qN80/yp0+/5m8LtnkOrBJpDgpxl0W/u5ZZDw0GYMfYYSx4pGqx4bM7V83Bcn732ptbCo607BB3bizGU1fIeylwTYlb4hHyoz7aUGPf+Hlbo+HulaGzN+5j5c7DQLi/+/7CMsbP2xI9PvSFhTHnlweCMe3k+UeqavfuL5Qrx37OhaNnxTzXef+c/FIyh0/j802xC247FXTnS+Q47eCUVJLlI1SIu/Q4pR29u2REH/c8tR0m8njKfVe6zjuh1tdp6f+Dl1XUHuJ1Te5V5lFrdituwAyPM9bXrNl6zXv+8tyt0e1C15fFtv1F9H5sOleM/Sy6r+/jVUHt3LgsjzQFlQeCrMvOI3P4ND78sqqb5Ze7wzX4Kau8p/F1fjG0Skvs/zrzbG501kiRo3FcdTFsiA//8yoCwRDtXWtuuoPecVr7NhwoCi+I3OJDPFB7yNYV0uV1PL8hRn5Ys6Z+NNZlh7sm7iso8zz+/JzwNL1lFVVl/95LiwF4bnYWg3t3AqAi8gVm9xVyqLic3MJSzu3aIfqFEi/Ef/X3lXQ7qS2nnNia5+dsJmvMTbROrxn0by7azppdh/l+/x5szi3kV4PPAuDnke6dy0YMISWFmMFpbsVlAVqnpyb8S0T8SyFeh7at0qLbz/6gL507tOHM005k8n1XsHrXYZ76NDyApnNGVYiffvIJrN9T4Pl69ZGWmtIoU9066nrp0oraQ7quibSaQ4cTwn/KJ7ZO8/wlsDyyAIZXO/qRisroCkltIsG7JbeI/k/MBuDLP1wfcy6Ee+isy85j+vocHrnBMGtjuPmlXevw30t5ZdAzxJ/4ZCMAn0Ta1J0Qdwx4KvxLYsfYYZ7Xef6omVzduxP/797LPI9L/enG5nHo+xf3YNA54RrbJZkdY/4HdM/19IuBvWp9nX49T671eHNX5OtatcjrJmRzu3fCSiC8MEhtvH5l7Csoi87V7jUlg3tBDCfEZ67P4XsvLWb8vK2s2X24xut/9vU+rn1mHvsLw1/sC7L2sz5BA5nmZ+2Pe2zG+pyYewESll9SUWNRlWSZYUEhfoxmPhi+EXppZkfO7ZrBsAu7cdmZHZn4i8v45L+qBg2dedqJ9X7Nxlj6LZFacvGO1PEroo1H7RhgTmTeF68l67a5BgiVRu4n/O+K3TWeC1X3Ex6dvI7tB4q59Mk5ANz91nJu/mvDF6iuj50Hi7nvH6v47ftrPY/nlZRz74QV0V+MiXLDc/P5+dvLE/qaiVJYWkHm8Gn0/eMsfvLmsmo9q1rwH/JRUIgfI9M1g88evpoRQ/sw48HBvByZ8nbQOZ244PSqXizukaHJUgNoiQ6X1L7UVuv0tFqPV3j0gd95sPYBPuPnba2xz13jr21SrevHzffc/97K3Tw/JzwDZXkgSObwabw8t6pnTqAyWON1na6b+wpKyT9SUaPpaNKyXXy+KZc3F3nPTVMZDNU6mKmiMujZnz9rX1HMqlRNLRgMcdXYz5myKpvJq7K5YNTM6LVfMmZO7LmufzNns3pvrbyS8rijc3MLSuOO+m0uCvEEOKtT+7g3mnp1CtfA3X88I28+D4Anb7sguu+09jWH/0tNdfXnL62j501RWdM3NdS2Punm3CLPL4FHJ6/j+TmbeXfFruh8P6/OrzrvunHzo68bCoWYsio72oxyqLicvo/P4sHI8UPF5czZuC9mJK2X52ZnMejPc+MG+a8nreaial0z3UKhULPMAlkRDLIn7wiPTF7Lb99fS1FZgGnr9lJaUVmj+WzngRL+tmAr67LzovXw3YeOxJzzb39fyT0TVpBfUkH24dh/i8ue+oxrn5kHwKqdhxj90YZmn/lSId7I/nTbhUDVvOav330JF3/rFHaMHcbt/XpEz8s8NRz253RunDVBk0VR6bH1wS88xuc3hNeyem5Pz6g5u6Tjd1O+YuGWcFuu+17EjoMlfPDlN2z8poCfvb2Ch99fyzORCcqyI5N5OQOShk9Zxy//vjIaziXllfzkjWXRed+dqQ2c99lfrbll6/4iVuw4xOyNsX3nq7v5r4u4YNTMGvsPFZdTFqjkgf9d4/mFdbTWZefFjAdwxmW4m/nKA8Ea4wgAvvvSIp76dBPfe2kxUzymZoaqrq3zN+9n4NNzmbZuLz97azn/WW0tzu+PX8qEJTtYveuw18s0GYV4IxvQ61TsmO/w3b7d2TF2GNef1yV67ITWafTp1gGoWt/y7iszo8efuOX86HZDBuAko7ravOvSHCF+rP77n2sA75uyQ19cyILIjU6vNU9fnrsl2nPGmaHxzUXbWbTlADc+v4DcglKGvbiIc34/naxIqD8z03Lna0ujN2KHPDufH7y6NPqaY6dvInP4tBrL7234poDi8krKA0G+3J0XnUun/xOz+feJq/jwy294esYmdh0s4U+ffs0s13w6oVCIzOHTPKdLdszZuI/l2w/xvZcW02vEp9H9byzcVuPcQDAUcy/Di7tZ6Y7xS8gcPo28kvLor+ZNkQneVu48xPys/dEeRQDvLKuaU6ikvJJ12XmUVlTy6vytfLBmT/R6vvP8glrLkAjqYtgE2tTSDnvPlZk8OmUdf7mjL8/NzuK2fqdzcc9T2H6gmJsu6BrtP+201vz93su4+62WeRPJDxqz62Zz65TRpkY/eWf6YIid2Mxx2ytLovudL8glW8MTkt3810XMjoxgdnOadX49aTVrRl5f4/iFo2dGv3DWjb4BIKbNfPBf5ka3Fz56LYP+PJeJv7gs+tob9xawIGs/b9x9CVedfRpHKirJ2lfIL/++kgzXeI1QKEReSYXnClEjpn5VY1917g4EzsjgYS8uin7RO+M93l68o9bXf/zjjWzJLeLyXh35Ylv4xvh3LgivXbApp5DSikrOHTmDa00n3r4n8V1DU5qqPccYkwls/+yzz+jRo0ddp0vE0BcWsnFvAW/+7BKe/PRrZj44mPV78klJSeG8bh3o/dj0mPM/+a+B0V4Qd17Sg/dWev9klOTjHnCWKNf16RKdxMzLDed1idb0vfz2ht48Mysr7nFHq7QUz5vK53fvwIZvChh3Z19+815sr5vb+5/O/632Hlnb3L7btzsfrw03o4259QIe+2A9EL//f22ys7MZMmQIwJnW2h3Vjyc0xI0xqcArQF+gDPiltXZL5FgmCvGjVlQW4HBxOWd0bOd53FnGbPZDg9l1qIQhfbowe+M+zjytHWd3zoge/+S/BvIfk1bx5s8u5YbnFtC7S3vG3Hohd762lIw26dEh7Ke1b82BonJap6V6Duo5oVXaMTdpiByvGiPEE90mfivQ1lp7BTAceDbBr3/cad8mPW6AA8x/5BpmPzSYc7pkMKRPuL39+vO6cHbn8NQAg845jQeGnMMFp5/Ewkf/hd5dMnhsWB/evucyLorMm35Ol/a8fc+l3DWgJ/MfuZZ/veQMlo0YwrM/6AvAtaYTZ3U6kX85tzOP3GgAGB/pSgkwYui50e3zu4fb+F9xHXffBxCRxEp0m/hAYAaAtfYLY8wlCX59qeZbp9Y+iGjiLwbU2PfLQVUjSv/5b5fTp1sGJ7drzbUmPA/603dcBMAt3+7O9gPF/HLQmZzcLtwFMhQKcWu/0+l4YmvW/uEGXluwlXuvOpPLe53K2t15DL2wG9sPFHNJZkcevr43z87O4pk7+nLfP1bxwHXn0Do9lU/W7uX3w/ow9IWF2H2FbHtxGnaYAAAGMElEQVRqKL1GfBr9aX5yu1YsHT6EPn+YAVT9JP/zHRfx6OR1QFVbKhD9JfHvg3tFl5u78fwuzNywjwtO7xCdAuGyzI4s33GIfj1PrjFVLYQHZLW0PsCSPG6KtJMnWqKbU94Aplhrp0ce7wJ6WWsDak6R6gKVQSpDoZgbv4eKy2mVlkJG21ZsyS2iVVpKzBfV3vwjnNKuNW1bpbF2dx7dTz4hpu/455v2cdXZp9EmPY3Jq7K5rk/nmC+g1bsOc/G3OlJSHuD5OZt56LreHCwuI/vwEc7q1J5FW/ZzW78e7DxYzJuLtjNiaB/WZedzuKSc008+gW0Hivle3+5k7Stkwzf53NavB8OnrKNPtw5sP1DMiW3SuPuKTP74yUZ6nHICj9xgGPriQkYM7cP4eVsxXTP4xcAzue2VJYwY2ocuHdowYupXvPmzS7nn7RXcO/BMcgtKmbI6m7/c0ZenZ2xiU04hT9x6ASM/WM9//8vZ5BSU8t7KbMbf1Z+X5m5hwzcF0S+wR2400ZuZzpcohJcmLA8E+eGlZ0RHmzrtzW69Op3Itv2xX2Q9O7ZjV6R7ovM6boPOOY2F1Ya0u53duT1bcmvOSOk49cTWHCyufZBWbdqkp1IWCPLTy7/FxC92AuGZRrMPH+GEVmlc3bsTPU9tx3V9unDna0vp3/Pk6AImv7m+N+NmZ5GemsLNF3Wr0R30il6nsrTaylNdOlTdQD6vW4eYZQprs+XJm0hvwMRlTd0mPg74wlr7XuRxtrW2R2Q7E4W4SFIKhUKkRLpzuLcdgcpgNMDc246KymB0wFxZoLJGj67yQHhCsVAoxJGKStq1brqOdaUVlbRJTyUlJcWz7KUVldGJ8tzX4cgvqeCkdq0a/P5N3Sa+GBgKYIy5HKi7n4+I+J47tKsHOBATfF61UXfweXXJdWaETElJadIAh/BMps41eZXdPdOp18jtYwnw+kj0v8ZU4HpjzBLCk/Hdk+DXFxERl4SGuLU2CNyXyNcUEZH4NOxeRMTHFOIiIj6mEBcR8TGFuIiIjzVlX500gJycnLrOExGRCFdmek6H2pQh3g3grrvuasK3FBFJGt2AGqtqNGWIrwAGAXsBTYMnIlI/aYQDfIXXwSabT1xERBJPNzZFRHysxS/PVttCE35mjFkD5EcebgdeA14AAsAsa+3jfr92Y8wA4Glr7TXGmLOBCUAIWA/cb60NGmNGAcMIX/eD1trl8c5tjmuoj2rX2R/4GNgcOTzeWvuu36/TGNMKeAvIBNoAY4CNJOFnGudas2mhn6sfauJJt9CEMaYtgLX2msh/9wCvAj8mPCf7gEgY+PbajTGPAm8AbSO7xgGPWWsHEZ5X55bINV4NDAB+CLwc79ymLPvR8LjO/sA412f7bjJcJ/AT4GCkrDcBL5Gknyne19piP9cWXxMnORea6Au0M8bMIvwZjAbaWGu3AhhjZgJDCN/M8Ou1bwVuByZGHl8MzI9sTwduACzhXx0hYJcxJt0Y0ynOuVObquBHyes6jTHmFsK1tgcJ/w37/TrfBya7HgdI3s803rW2yM/VDzXxDlQ1OwBUGmP88OVTmxLgGeBGwhOGvR3Z5ygETsLH126tnQK4lyFPifyxQ/zrc/Z7ndsieVzncuARa+1gYBswiuS4ziJrbaExJoNwwD1G8n6mXtfaYj9XP4R4AZDhepxqrQ00V2ESJAv4h7U2ZK3NIvyH0NF1PAPII7mu3d0mGO/6nP1e5/rFVGvtKmcb6EeSXKcx5gxgLjDRWvsOSfyZelxri/1c/RDiybjQxL1E2reNMd2BdkCxMeYsY0wK4Rr6QpLr2tcYY66JbN9E1fXdaIxJNcb0JPwldSDOuX4x0xhzWWR7CLCKJLhOY0wXYBbwO2vtW5HdSfmZxrnWFvu5+uGneTIuNPEmMMEYs4jwHex7CX97TyLcsX+WtXaZMWYFyXPtDwOvG2NaA18Dk621lcaYhcBSwhWK++Od2xwFbqD/AF4yxpQDOcCvrLUFSXCdI4BTgJHGmJGRfQ8ALybhZ+p1rb8Bnm+Jn6sG+4iI+JgfmlNERCQOhbiIiI8pxEVEfEwhLiLiYwpxEREfU4iLiPiYQlxExMcU4iIiPvb/AQR87wub5lePAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " s tell you things that now \n",
      "When I leave you behind \n",
      "It plays on my mind now honey \n",
      "What am ve l thats or maybes \n",
      "When I'm falling right now \n",
      "So let's do it right now onows \n",
      "I let you in where no-one  \n",
      "----\n",
      "iter 2600, loss 12.486313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9b2020035120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                 for ch in data[pointer + 1: pointer + T_steps + 1]])\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_h_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_C_prev\u001b[0m \u001b[1;33m=\u001b[0m         \u001b[0mforward_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_h_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_C_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-d6486bec3269>\u001b[0m in \u001b[0;36mforward_backward\u001b[1;34m(inputs, targets, h_prev, C_prev)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mC_bar_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         v_s[t], y_s[t]) = \\\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Loss for at t\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-7383994b8dc3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(x, h_prev, C_prev, p)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mH_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mC_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mH_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Reset\n",
    "    if pointer + T_steps >= len(data) or iteration == 0:\n",
    "        g_h_prev = np.zeros((H_size, 1))\n",
    "        g_C_prev = np.zeros((H_size, 1))\n",
    "        pointer = 0\n",
    "\n",
    "\n",
    "    inputs = ([char_to_idx[ch] \n",
    "               for ch in data[pointer: pointer + T_steps]])\n",
    "    targets = ([char_to_idx[ch] \n",
    "                for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "    loss, g_h_prev, g_C_prev = \\\n",
    "        forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # Print every hundred steps\n",
    "    if iteration % 100 == 0:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "    update_paramters()\n",
    "\n",
    "    plot_iter = np.append(plot_iter, [iteration])\n",
    "    plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "    pointer += T_steps\n",
    "    iteration += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
